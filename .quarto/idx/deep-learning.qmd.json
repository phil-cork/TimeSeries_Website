{"title":"Deep Learning","markdown":{"yaml":{"title":"Deep Learning","editor":"visual","format":{"html":{"toc":false}},"execute":{"echo":false}},"headingText":"Neural Networks","containsRefs":false,"markdown":"\n\nSince our [multivariate analysis](arimax.html) using Magic: the Gathering cards to model Hasbro's stock pirce is constrained by a lack of daily data, we return to consider how well we can predict the stock price. In this iteration, however, we investigate the effectiveness of three neural network implementations to see if more sophisticated modeling techniques can improve results from our original [ARMA model](arima.html).\n\nAs with most machine learning techniques, there is a trade-off between interpretability and predictive power. In this case, we consider whether the more complicated neural nets outperform our simpler models enough to warrant their use. Given the goal of predicting a stock price, there is less prioritization given to being able to explain the model and a higher focus on its output.\n\nBefore diving into the modeling techniques, we once again review the Hasbro stock price over the last several years. Even in viewing a scaled version of the data, we can clearly observe the market collapse in early 2020 due to the COVID-19 pandemic as well as the tumultuous performance of the stock across 2022, [as previously explored](data-visualization.html). We also denote the training data (blue) and test data (red) that will be used in building and evaluating the neural networks.\n\n\\\n\n![](images/deep-learning/stock-price.png){fig-align=\"center\"}\n\n\\\n\n\nWe consider three neutral networks designs. In each case, the models are built with and without L2 regularization to evaluate the reguarization's impact on overfitting and the error term. In each case, we set our partition to predict five days out to represent a week's worth of trading days.\n\n![](images/deep-learning/partitions.png)\\\n\n### Recurrent Neural Network\n\nThe first is a Recurrent Neural Network (RNN), a vanilla neural net implementation with three hidden layers, one dense layer, and a hyperbolic tangent activation function.\n\n::: panel-tabset\n#### RNN\n\n![](images/deep-learning/rnn.png){fig-align=\"center\"}\n\n#### RNN w/ L2 Regularization\n\n![](images/deep-learning/rnn_reg.png){fig-align=\"center\"}\n:::\n\n\\\n\nFrom plotting the loss function over the coursre of each epoch, we see clearly the benefit of using regularization with the RNN model. The first graph shows a sharper descent in the loss term, but also depicts the model overfitting to the training data.\n\n\\\n\n| Model     | Train RSME | Test RSME |\n|-----------|------------|-----------|\n| RNN       | 0.03303    | 0.03158   |\n| RNN w/ L2 | 0.02831    | 0.02568   |\n\nThe regularized implementation of the Recurrent Neural Network also performs better on both the training and testing data.\n\n\\\n\n### Gated Recurrent Unit\n\nWe then construct a Gated Recurrent Unit (GRU) neural network. To fight overfitting, this model includes a recurrent dropout mask which helps sever correlation between training data and the layer of the neural network that its exposed to. GRUs are noted as working well with small data. With only 1,000 observations across the train and test data, this could prove to be effective for the task at hand.\n\n\\\n\n::: panel-tabset\n#### GRU\n\n![](images/deep-learning/gru.png){fig-align=\"center\"}\n\n#### GRU w/ L2 Regularization\n\n![](images/deep-learning/gru_reg.png){fig-align=\"center\"}\n:::\n\n\\\n\nSimilarly, with the GRU model, we denote that the regularized model starts with a higher term, but eventually and more gradually converged to a similar results.\n\n\\\n\n| Model     | Train RSME | Test RSME |\n|-----------|------------|-----------|\n| GRU       | 0.02908    | 0.02713   |\n| GRU w/ L2 | 0.02871    | 0.02954   |\n\nUnlike with the Recurrent Neutral Network, we do not see as dramatic a difference between the default model and the regularized iteration. Instead, the models have very similar error terms, with the default GRU slightly out performing on both the train and test data. These results are largely in line with the regularized RNN model.\n\n\\\n\n### Long Short Term Memory\n\nFinally, we build a Long Short Term Memory (LSTM) neural network. While LSTM models are most often used with text data, they can work broadly with any sequential data series and so it applies well to our daily stock price task.\n\n\\\n\n::: panel-tabset\n#### LTSM\n\n![](images/deep-learning/ltsm.png){fig-align=\"center\"}\n\n#### LTSM w/ L2 Regularization\n\n![](images/deep-learning/ltsm_reg.png){fig-align=\"center\"}\n:::\n\n\\\n\nIn contrast to the previous iterations, we note for both models higher initial error terms and the more gradual convergences over the course of the training epochs.\n\n\\\n\n| Model      | Train RSME | Test RSME |\n|------------|------------|-----------|\n| LTSM       | 0.03884    | 0.02855   |\n| LTSM w/ L2 | 0.03851    | 0.03263   |\n\nThe performance terms bear out the expected results from the plots. With higher train and test RSME values than the RNN or GRU alternatives, its possible these models underfit the data and thus are not the most representative option.\n\n\\\n\n### Selecting a Neutral Network Model\n\nHaving explored three difference architectures and their regularized counterparts, we select the regularized Recurrent Neural Network as it features the optimized RMSE and does not appear to neither overfit nor underfit the training data as much as other options.\n\nWe can then plot the actual and predicted values to discover where the model performs well and where it struggles to model Hasbro's stock price.\n\n![](images/deep-learning/actual-preds-plot.png){fig-align=\"center\"}\nIn this plot, the green line depicts the predicted values for each daily Hasbro stock price. Surprisingly, the model follows and captures the COVID-19 pandemic effect rather well. The one area where it falls off in performance is the all-time highs reached in late 2019, where the model under-estimates the stock price for a few weeks in a row. Outside of this one area, the model performs well, despite the extensive degree of noise in the dataset.\n\n\\\n\n## Conclusion\n\nDoes the added complexity of the neutral network sufficiently improve its predictive capabilities over the ARMA model? The selected ARMA model, which narrowly beat out a more naive benchmark, had a RSME of 1.8. As such, any of these neural networks offer impressive improvements in predicting Hasbro's stock price, particularly when considered that it is predicting further into the future than the ARMA model as well. Comparing this model to the ARIMA model is not quite an equivalent comparison because the ARIMA is limited to a weekly time series, but it also featured a similar RMSE of 0.027. \n\nBecause inference and interpretation are less crucial than precise forecasting in this instance, the neural network is the clearly superior model compared to the ARMA iteration. However, the ARIMAX model performing similarly while also working with less granular data suggests that were the data constraints of using Magic: the Gathering products as a model feature could be overcome, it could prove to be a more powerful model than deploying a neutral network with only past stock prices as means to predict future values.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":false,"output-file":"deep-learning.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.313","editor":"visual","theme":"lumen","title":"Deep Learning"},"extensions":{"book":{"multiFile":true}}}}}