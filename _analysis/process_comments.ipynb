{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prcork/Georgetown/Spring2023/TimeSeries/ANLY560_website\n"
     ]
    }
   ],
   "source": [
    "# import reddit api wrapper\n",
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sqlite3\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Path hack\n",
    "import os\n",
    "# change directory from the current Analysis folder to the top level folder for easier navigation\n",
    "os.chdir('../')\n",
    "# confirm we're one level up\n",
    "print(os.getcwd())\n",
    "\n",
    "# note that the %load_ext autoreload line only needs to be be run once\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from _functions.get_reddit_data import get_submissions, get_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"_data/magic_reddit.db\") as conn:\n",
    "\n",
    "    submissions = pd.read_sql_query(\"SELECT submission_id FROM submissions\", conn)\n",
    "    print(\"Total Submissions: \" + str(len(submissions)))\n",
    "\n",
    "    # pull in all submission ids that have previously been crawled\n",
    "    df = pd.read_sql_query(\"SELECT * FROM comments\", conn)\n",
    "    crawled_ids = df['submission_id'].unique().tolist()\n",
    "    print(\"Crawled Submissions: \" + str(len(crawled_ids)))\n",
    "\n",
    "    # yields the elements in all_ids not in crawled_ids\n",
    "    remaining_ids = np.setdiff1d(submissions.submission_id, crawled_ids)\n",
    "    print(\"Submissions remaining: \" + str(len(remaining_ids)))\n",
    "\n",
    "    for each_id in remaining_ids:\n",
    "\n",
    "        comment_df = get_comments(each_id)\n",
    "        \n",
    "        comment_df.to_sql(name='comments', con=conn, if_exists='append', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
