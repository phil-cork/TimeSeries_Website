---
title: "ARMA/ARIMA/SARIMA Models"
editor: visual
format:
  html:
    toc: false
execute:
  echo: false
---

```{r, echo=FALSE,message=FALSE,warning=FALSE}

library(tidyverse)
library(forecast)
library(astsa) 
library(xts)
library(tseries)
library(fpp2)
library(fma)
library(lubridate)
library(TSstudio)
library(quantmod)
library(tidyquant)
library(plotly)
library(jtools)

options(scipen = 999999999)
```

In the first attempt to model Hasbro's stock price, we found that using only past prices to forecast future values proved to be difficult. The optimal model only barely outperformed a simple benchmark method. This reality is unsurprising. Hasbro is large corporation in a dynamic trading environment built on imperfect knowledge and imperfect markets. To further improve the model, we consider how additional features may improve the model and its predictive capabilities.

\

## Can Magic the Gathering help predict Hasbro's stock price?

![](images/magic-cards.jpg){width="961"}

\

As described in this project's [introduction](/introduction.html), Magic the Gathering is a valuable method for investigating Hasbro's stock value because it has become an increasingly important part of their portfolio. It recently became Hasbro's first [\$1 billion brand](https://hasbro.gcs-web.com/news-releases/news-release-details/hasbro-reports-third-quarter-financial-results). While other subsidiaries report net losses, Wizards of the Coast, producer of Magic the Gathering as well as Dungeons and Dragons, has continued to report strong profits, driving much of Hasbro's revenue.

Further, the secondary market for Magic the Gathering cards offers a unique way to potentially enhance the model's predictive capability. Given that Magic is such an important part of Hasbro's portfolio, it's possible that Magic card prices could inform the stock's price movement. For example, in quarters when popular Magic products are sold, those products are likely to maintain a higher price on the secondary market. Then, the resulting profit from these products being sold by Wizards of the Coast into the secondary market would be reported to Hasbro investors, potentially increasing the stock's value.

This context sets the stage for selecting features and testing the central question - can Magic the Gathering predict Hasbro's stock price?

\

## Selecting Magic the Gathering Model Features

Given that there are more than 20,000 Magic cards released over the last thirty years, a vast majority of these data points would only introduce noise into the model. Thus, it is more helpful to consider the sealed products in which cards are purchased. See the [data visualization](/data-visualization.html) page for more details on these products.

When considering these products, it is unlikely that any products outside those currently being produced by Wizards of the Coast would impact Hasbro's valuation. While print run data for Magic card is not publicly available, there is a general consensus that each standard set of Magic cards is in production for 12-18 months and are generally printed to demand. Exceptions to this rule include limited print run products such as Collector Booster Boxes and Remastered sets, which are only printed once or in a much more constrained quantity in multiple waves of printing.

To estimate how the Magic the Gathering secondary market could inform Hasbro's stock price, we include the weekly prices for the sealed Magic products from 2021. This subset of products ensures that all were still in production and thus relevant to Hasbro's financial performance.

Even in working with sealed products, the volume of product releases in 2022 creates dozens of potential features, all of which enter the market at different times. To simplify the secondary market data further into a more equivalent comparison, all of the products are aggregated together by the product type and the average price value is taken for however many products are available to purchase in a given week.

\

```{r, message=FALSE,warning=FALSE}

# import manually scraped data for magic the gathering sealed products
sealed <- read.csv("../_data/sealed_market_value.csv")
sealed$week_start <- as.Date(sealed$week_start)
sealed$week_end <- as.Date(sealed$week_end)

# aggregate by box type to create 3 time series columns by week and year
by_box <- sealed %>%
  group_by(week_start, box_type) %>%
  summarize(avg_price = round(mean(price),2)) %>%
  mutate(week = week(ymd(week_start)),
         week = case_when(
           week > 6 ~ week,
           TRUE ~ week + 52),
         logged_price = log(avg_price)
         ) %>%
  rename(id = box_type) %>%
  ungroup() %>%
  select(-week_start)

# plot the average price by box type over time
ggplot(by_box, aes(x=week, y=avg_price, color=id)) + 
  geom_line() + 
  theme_minimal() + 
  labs(x="Week", y="Average Product Price", color="Product Type",
       title="Sealed Magic Prices Over Time")
```

\

## Selecting Other Model Features

We also include Hasbro's trade volume to capture the amount of activity at a given price point. The data is also transformed from daily prices to average weekly price to match the frequency of the available Magic the Gathering product data. The average is chosen as the aggregation method because while there is volatility present in the stock price, there are not such huge changes on a daily basis as to create outliers that would warp the mean of the week's adjusted stock prices.

```{r, message=FALSE,warning=FALSE}
# import Hasbro stock price and trade volume
HAS <- tq_get("HAS", get = "stock.prices", from = "2022-02-14", to = "2023-01-23")
HAS$date<-as.Date(HAS$date)
#has_price_ts <- ts(HAS$adjusted, frequency=365.25, start=2022)
#has_vol_ts <- ts(HAS$volume, frequency=362.25, start=2022)

# convert Hasbro to weekly, aggregating volume and price
has_week <- HAS %>%
  mutate(week = week(ymd(date)),
         week = case_when(
           week > 6 ~ week,
           TRUE ~ week + 52),
         id = "Hasbro"
         ) %>%
  group_by(id, week) %>%
  summarize(avg_volume = round(mean(volume),2),
            avg_price = round(mean(adjusted),2),
            logged_price = log(avg_price), 
            logged_volume = log(avg_volume))

```

Finally, we include the S&P500 average price to capture the overall market's trajectory as a potential extraneous explanation to Hasbro's price. It is worth nothing that while Hasbro is in the S&P500, they are listed as [the 487th inclusion by weight](https://www.slickcharts.com/sp500), so any potential information leakage is likely to be insignificant when the estimated coefficients for the S&P500's price will be driven by much larger corporations and market forces.

```{r, message=FALSE,warning=FALSE}
# import S&P500
sp500 <- tq_get("^GSPC", get = "stock.prices", from = "2022-02-14", to = "2023-01-23")
sp500$date <- as.Date(sp500$date)

# convert S&P500 to weekly, aggregating volume and price
sp_week <- sp500 %>%
  mutate(week = week(ymd(date)),
         week = case_when(
           week > 6 ~ week,
           TRUE ~ week + 52),
         id = "SP500"
         ) %>%
  group_by(id, week) %>%
  summarize(avg_volume = round(mean(volume),2),
            avg_price = round(mean(adjusted),2),
            logged_price = log(avg_price), 
            logged_volume = log(avg_volume))
```

Finally, we compare our target - Hasbro's weekly average stock price - with the Magic products and other features that will be added to the model. Because the prices vary in scale, all average prices are logged to decrease the influence of any one variable being due simply to the instrument's valuation.

\

```{r}

all_prices <- bind_rows(by_box, has_week, sp_week) %>%
  mutate(logged_price = log(avg_price), 
         logged_volume = log(avg_volume)) %>%
  filter(week < 56)

all_prices_ts <- all_prices %>%
  select(week, logged_price, id) %>%
  pivot_wider(names_from=id, values_from=logged_price)

just_logged_volume <- all_prices %>%
  filter(id=="Hasbro") %>%
  select(logged_volume) %>%
  rename(Volume = logged_volume)

all_prices_ts <- bind_cols(all_prices_ts, just_logged_volume)

# export data for use in GARCH Model page without rerunning all processing code
# write.csv(all_prices_ts, "_data/arimax-model-data.csv", row.names = FALSE)

all_prices_ts <- ts(all_prices_ts)

autoplot(all_prices_ts[,c(2:7)], facet=TRUE) + 
  theme_minimal() + 
  labs(x="Week", y="Avg Price, Logged",
       title="Comparing Model Features")
```

\

## ARIMAX Model Fitting

After preparing the multivariate features, we use both the `auto.arima` function and a manual model selection process to determine the best parameters for modeling and forecasting Hasbro's stock price.

The results of each model's implementation and the step by step stages of the model selection process are included below.

\

```{r}

# split into features and target

xreg <- cbind(collector = all_prices_ts[, "Collector"],
              draft = all_prices_ts[, "Draft"],
              set = all_prices_ts[, "Set"],
              sp = all_prices_ts[, "SP500"],
              volume = all_prices_ts[, "Volume"])
```

### Automatic ARIMA Model Selection

```{r}
# auto.arima
fit <- auto.arima(all_prices_ts[, "Hasbro"], xreg = xreg)
#summary(fit)
```

| ARMA(1,1)     | Estimate | Standard Error |
|---------------|----------|----------------|
| **ar1**       | 0.9743   | 0.0276         |
| **ma1**       | 0.421    | 0.158          |
| **collector** | -0.077   | 0.094          |
| **draft**     | 0.0603   | 0.0626         |
| **set**       | -.4006   | 0.1546         |
| **s&p500**    | 0.7981   | 0.0901         |
| **volume**    | -0.0204  | 0.0089         |

\

The auto-generated model results in a ARMA(1,1) design. The model has an AIC of -191.99 and a BIC of -176.85. From comparing the estimates and standard errors, we can note that the ar1, ma1, set booster, s&p500, and volume variables are statistically significant.

We next implement a manual model selection process starting with a general linear regression and then introducing several ARIMA model options for capturing the residuals of the regression's fit.

\

### Manual ARIMA Model Selection

```{r}
# manual
fit.reg <- lm(Hasbro ~ Collector+Set+Draft+SP500+Volume, data=all_prices_ts)
```

|             | Estimate | Standard Error | T Value | P             |
|-------------|----------|----------------|---------|---------------|
| (Intercept) | 3.0551   | 2.5889         | 1.180   | 0.2444        |
| Collector   | 0.8785   | 0.2162         | 4.063   | 0.0002 \*\*\* |
| Set         | -0.2374  | 0.3987         | -0.595  | 0.5547        |
| Draft       | -0.1426  | 0.1122         | -1.271  | 0.2105        |
| S&P500      | 0.0323   | 0.3217         | 0.101   | 0.9203        |
| Volume      | -0.1379  | 0.0322         | -4.276  | 0.0001 \*\*\* |

\

Interestingly, when computing only the linear regression, the Collector Box products and Hasbro trade volume prove to be statistically significant. However, this model does not taking into account the endogenous auto-correlation present in the time series. To address this shortcoming, we model the residuals of the model using an ARIMA model.

\

```{r}
res.fit <- ts(residuals(fit.reg))
```

::: panel-tabset
## ACF

```{r}
par(xpd=NA,oma=c(0,0,2,0))
acf(res.fit)

```

## PACF

```{r}
par(xpd=NA,oma=c(0,0,2,0))
Pacf(res.fit)
```
:::

\

Based on the ACF and PACF plot of the linear regression's residuals, it looks like only a few values are statistically significant, offering a subset of values to consider for implementing the ARIMAX model. From the PACF, 1 is the only statistically significant value, but 2 is nearly above the threshold, so it will be considered as well. In the ACF plot, the first three lags are statistically significant and will be options for the model's parameters.

```{r}

i=1
temp= data.frame()
ls=matrix(rep(NA,6*24),nrow=24) # roughly nrow = 3x4x2


for (p in c(0,1,2))
{
  for(q in c(0,1,2,3)) 
  {
    for(d in c(0,1))
    {
        model<- Arima(res.fit, order=c(p,d,q), include.drift=FALSE) 
        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)
        i=i+1
    }
  }
}

temp= as.data.frame(ls)
names(temp)= c("p","d","q","AIC","BIC","AICc")
```

Two models stand out from these results. ARIMA(2,0,3) minimizes the AIC with a low BIC, while ARIMA(1,0,0) minimizes the BIC with a low AIC as well. With these two models and the auto-generated ARIMA(1,0,1) model, we continue with model diagnostics and evaluation through cross-validation.

\

## Model Diagnostics

::: panel-tabset
## ARIMA(2,0,3)

```{r}
# model diagnostics

set.seed(1234)

model_output_m1 <- capture.output(sarima(res.fit, 2,0,3))
```

## ARIMA(1,0,0)

```{r}
model_output_m2 <- capture.output(sarima(res.fit, 1,0,0)) 
```

## ARIMA(1,0,1)

```{r}
model_output_a <- capture.output(sarima(res.fit, 1,0,1))
```
:::

\

All three models report very similar results, potentially pointing to the difficulty of the modeling task and noise inherent in the financial data we're working with. A few smaller differences stand out - ARIMA(1,0,0) includes a point where the p-values in the Ljung-Box statistics plot is not above the statistically significant threshold. Only ARIMA(2,0,3) includes an ACF plot without any statistically significant lags, but even this is likely a minimal difference between it and the others.

As such, we are inclined to proceed with all three models for future consideration. Next we will use cross validation to further evaluate the models.

\

## Model Cross Validation

```{r,message=FALSE,warning=FALSE}
k <- 7
n <- length(res.fit)
mae1 <- mae2 <- mae3 <- matrix(NA,n-k,4)
st <- tsp(res.fit)[1]+(k-2)

for(i in 1:(n-k))
{
  xshort <- window(res.fit, end=st + i)
  xnext <- window(res.fit, start=st + (i+1), end=st + (i+4))
  
  fit1 <- Arima(xshort, order=c(2,0,3),
      include.drift=TRUE, method="ML")
  fcast1 <- forecast(fit1, h=4)
  
  fit2 <- Arima(xshort, order=c(1,0,0),
      include.drift=TRUE, method="ML")
  fcast2 <- forecast(fit2, h=4)
  
  fit3 <- Arima(xshort, order=c(1,0,1),
      include.drift=TRUE, method="ML")
  fcast3 <- forecast(fit3, h=4)
  
  mae1[i,1:length(xnext)] <-  sqrt((fcast1$mean-xnext)^2)
  mae2[i,1:length(xnext)] <-  sqrt((fcast2$mean-xnext)^2)
  mae3[i,1:length(xnext)] <-  sqrt((fcast3$mean-xnext)^2)
}

plot(1:4, colMeans(mae1,na.rm=TRUE), type="l", col=2, xlab="horizon", 
     ylab="RMSE", main="RMSE of Forecasting Fitted Residuals", ylim=c(0.04,0.12))
lines(1:4, colMeans(mae2,na.rm=TRUE), type="l",col=3)
lines(1:4, colMeans(mae3,na.rm=TRUE), type="l",col=4)
legend("topleft",legend=c("(2,0,3)","(1,0,0)","(1,0,1)"),col=2:4,lty=1)


```

\

Considering the three models, we observe the most complicated model, ARIMA(2,0,3) has the highest RSME at each prediction point. The other two models have more similar RSME plots, with the auto-generated model, ARIMA(1,0,1) having a slightly higher RSME for the first horizon, but having a more gradual increase than the other model, ARIMA(1,1,0), which has the lowest RMSE for the first three horizons and is only slightly less accurate on the fourth horizon than the auto-generated model.

With these results, along with the previous model diagnostic steps, we proceed with the auto-generated ARIMA(1,0,1) model for forecasting with exogenous variables.

\

## Fitting the Chosen Model

```{r}
# fit the model using Arima(), obtain the summary fit, write the equation
final_fit <- Arima(all_prices_ts[, "Hasbro"],order=c(1,0,1), xreg=xreg)
# summary(final_fit)
```

After manually investigating models, we select the ARIMAX(1,1) model for its simplicity, optimized AIC and BIC values, and smoother predictive performance over the forecasting horizon explored above. It also features an RSME of 0.027, a noteable improvement over the 1.8 RSME of the ARMA model previously considered.

The coefficients are slightly different from the original auto-arima model's output due to the inclusion of the intercept in these results. The same variables remain statistically significant, however.

\

| ARMA(1,1)     | Estimate | Standard Error |
|---------------|----------|----------------|
| **ar1**       | 0.9678   | 0.0327         |
| **ma1**       | 0.4623   | 0.1448         |
| **intercept** | -1.8063  | 1.4555         |
| **collector** | -0.409   | 0.0970         |
| **draft**     | 0.0614   | 0.0619         |
| **set**       | -0.3768  | 0.1521         |
| **s&p500**    | 0.9754   | 0.1638         |
| **volume**    | -0.0200  | 0.0086         |

\

In general, a ARIMAX model with five exogenous variables and an ARMA(1,1) structure would take the form:

$y_t = \beta_1x_{1t} + \beta_2x_{2t} + \beta_3x_{3t} + \beta_4x_{4t} + \beta_5x_{5t} + \phi_1y_{t-1} - \theta_1z_{t-1} + z_t$

\

In this fitted model, this equation would be:

$y_t = -0.409collector_t + 0.0614draft_t -0.3768set_t + 0.9754sp500_t - 0.02volume_t + 0.9678y_{t-1} - 0.4623z_{t-1} - 1.8063$

\

## Forecasting Hasbro's Stock Price

```{r}
# predict other values using auto.arima()
# 

col_fit <- auto.arima(all_prices_ts[,'Collector'])
fcol <- forecast(col_fit)

dft_fit <- auto.arima(all_prices_ts[,'Draft'])
fdft <- forecast(dft_fit)

set_fit <- auto.arima(all_prices_ts[,'Set'])
fset <- forecast(set_fit)

sp_fit <- auto.arima(all_prices_ts[,'SP500'])
fsp <- forecast(sp_fit)

vol_fit <- auto.arima(all_prices_ts[,'Volume'])
fvol <- forecast(vol_fit)

```

```{r}

fxreg <- cbind(collector = fcol$mean,
              draft = fdft$mean,
              set = fset$mean,
              sp = fsp$mean,
              volume = fvol$mean)

fcast <- forecast(final_fit, xreg=fxreg)
autoplot(fcast) + xlab("Week") +
  ylab("Avg. Weekly Price, Logged") + 
  theme_minimal()
```

\

Ultimately, the model's forecast appears slightly optimistic for Hasbro's stock with a gradual upward trajectory. The width of the confidence bounds highlight the uncertainty of these predictions in which Hasbro's price could continue falling to lows not seen in years or could bounce back and recover the lost value from the last fifteen weeks, or anything in between. While these results improve upon the default ARIMA model used in our [uni-variate analysis](/models.html), the model remains limited in its usability.

As another observation from the model, it is interesting to note the statistically significant variables. Within the final ARIMAX model, the coefficient for the logged weekly average price of Set boosters boxes, the logged weekly average price of the S&P500, and the logged average weekly trade volume of Hasbro were all statistically significant Of course, because of the auto-correlated lag variables included in the model, these can not be interpreted as directly as in their OLS counterpart, but their presence is helpful in identifying potential sources of predictive power.

The resulting forecast and significant variables indicate the market's broad forces and the activity surrounding Hasbro's stock may be more powerful in forecasting than the secondary market data. This is unsurprising, but it is encouraging to see one variable representing Magic the Gathering as statistically significant as well.

Finally, it's worth remembering that there are no shortage of other variables within the error term of the model. These missing features likely introduce endogeneity to the model. Examples of these spurious variables include the unknown volume of Magic the Gathering products sold in a given week, other economic indicators such as treasury yields or inflation rates, the sentiment of investors over time, and other related stock prices, such as rival toy company, Mattel.

\

## Conclusion

In this exercise, we have justified the use of Magic the Gathering secondary market data in multivariate analysis for predicting Hasbro's stock based on it's importance to Hasbro's overall revenue and profitability. We evaluated a number of ARIMA models, both auto-generated and manually identified. We conducted model diagnostics and cross-validation to select a best-fit model. We then used this model along with the identified exogenous variables to forecast Hasbro's future stock prices. The results improved upon simpler processes, but can be further improved. In future steps, we will analyze and model Hasbro's volatility with ARCH models to better capture the stock's performance.
